{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CreateWordEmbeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCCtrfI3xTG9ALy3sppW/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjdolan/DS_Portfolio/blob/main/CreateWordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Word Embeddings**"
      ],
      "metadata": {
        "id": "gm927ZwN8ZOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install text-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_VXLHSD-7I0",
        "outputId": "7df32c6a-f339-4132-bbf0-29eb680a0419"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting text-preprocessing\n",
            "  Downloading text_preprocessing-0.1.0-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting unittest-xml-reporting\n",
            "  Downloading unittest_xml_reporting-3.2.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting names-dataset\n",
            "  Downloading names-dataset-3.1.0.tar.gz (58.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.7)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting pycountry\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 38.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry->names-dataset->text-preprocessing) (57.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from unittest-xml-reporting->text-preprocessing) (4.2.6)\n",
            "Building wheels for collected packages: names-dataset, pycountry\n",
            "  Building wheel for names-dataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names-dataset: filename=names_dataset-3.1.0-py3-none-any.whl size=116832781 sha256=6d05beabe4a12e2fbd8afe4a544aa9cf5fac2be6c60ed46adfa44f07c7b09c3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/59/19/b3a2b3fe7477148739ddd6e588ea3b2fb713188c8e5e2f31ac\n",
            "  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=dac75b469772a86973e23b1c55bd604020518f49cdb65c0287a6d6eb8c45533c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n",
            "Successfully built names-dataset pycountry\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, pycountry, unittest-xml-reporting, pyspellchecker, names-dataset, contractions, text-preprocessing\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 names-dataset-3.1.0 pyahocorasick-1.4.4 pycountry-22.3.5 pyspellchecker-0.6.3 text-preprocessing-0.1.0 textsearch-0.0.21 unittest-xml-reporting-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A9xqzrix8WzM"
      },
      "outputs": [],
      "source": [
        "# input text\n",
        "\n",
        "texts = ['The future king is the prince', 'Daughter is the princess',\n",
        "         'Son is the prince', 'Only a man can be a king',\n",
        "         'Only a woman can be a queen', 'The princess will be a queen',\n",
        "         'Queen and king rule the realm', 'The prince is a strong man',\n",
        "         'The princess is a beautiful woman', \n",
        "         'The royal family is the king and queen and their children',\n",
        "         'Prince is only a boy now', 'A boy will be a man']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(\n",
        "    string: str, \n",
        "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
        "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
        "    \"\"\"\n",
        "    A method to clean text \n",
        "    \"\"\"\n",
        "    # Cleaning the urls\n",
        "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
        "\n",
        "    # Cleaning the html elements\n",
        "    string = re.sub(r'<.*?>', '', string)\n",
        "\n",
        "    # Removing the punctuations\n",
        "    for x in string.lower(): \n",
        "        if x in punctuations: \n",
        "            string = string.replace(x, \"\") \n",
        "\n",
        "    # Converting the text to lower\n",
        "    string = string.lower()\n",
        "\n",
        "    # Removing stop words\n",
        "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
        "\n",
        "    # Cleaning the whitespaces\n",
        "    string = re.sub(r'\\s+', ' ', string).strip()\n",
        "\n",
        "    return string        \n"
      ],
      "metadata": {
        "id": "jE-fIUCM8hQK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from text_preprocessing import preprocess_text\n",
        "\n",
        "# Defining the window for context\n",
        "window = 2\n",
        "\n",
        "# Creating a placeholder for the scanning of the word list\n",
        "word_lists = []\n",
        "all_text = []\n",
        "\n",
        "for text in texts:\n",
        "\n",
        "    # Cleaning the text\n",
        "    text = text_preprocessing(text)\n",
        "\n",
        "    # Appending to the all text list\n",
        "    all_text += text \n",
        "\n",
        "    # Creating a context dictionary\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            # Getting the context that is ahead by *window* words\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
        "            # Getting the context that is behind by *window* words    \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])"
      ],
      "metadata": {
        "id": "IBUL-hBP8hS4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unique_word_dict(text:list) -> dict:\n",
        "    \"\"\"\n",
        "    A method that creates a dictionary where the keys are unique words\n",
        "    and key values are indices\n",
        "    \"\"\"\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\n",
        "    words = list(set(text))\n",
        "    words.sort()\n",
        "\n",
        "    # Creating the dictionary for the unique words\n",
        "    unique_word_dict = {}\n",
        "    for i, word in enumerate(words):\n",
        "        unique_word_dict.update({\n",
        "            word: i\n",
        "        })\n",
        "\n",
        "    return unique_word_dict "
      ],
      "metadata": {
        "id": "rG1Hs1ft8hVC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_word_dict = {\n",
        " 'beautiful': 0,\n",
        " 'boy': 1,\n",
        " 'can': 2,\n",
        " 'children': 3,\n",
        " 'daughter': 4,\n",
        " 'family': 5,\n",
        " 'future': 6,\n",
        " 'king': 7,\n",
        " 'man': 8,\n",
        " 'now': 9,\n",
        " 'only': 10,\n",
        " 'prince': 11,\n",
        " 'princess': 12,\n",
        " 'queen': 13,\n",
        " 'realm': 14,\n",
        " 'royal': 15,\n",
        " 'rule': 16,\n",
        " 'son': 17,\n",
        " 'strong': 18,\n",
        " 'their': 19,\n",
        " 'woman': 20\n",
        "}"
      ],
      "metadata": {
        "id": "XW_UaJ129w8d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Defining the number of features (unique words)\n",
        "n_words = len(unique_word_dict)\n",
        "\n",
        "# Getting all the unique words \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "# Creating the X and Y matrices using one hot encoding\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    # Getting the indices\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        "\n",
        "    # Creating the placeholders   \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    # One hot encoding the main word\n",
        "    X_row[main_word_index] = 1\n",
        "\n",
        "    # One hot encoding the Y matrix words \n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    # Appending to the main matrices\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "# Converting the matrices into an array\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAxZAekT8hYf",
        "outputId": "35fff663-051d-47ce-9984-fb853794dfe3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "PSWKozgPHBNs",
        "outputId": "ab5f6e89-6e6f-41e1-f4df-1d65a42e156b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c38ad521c428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning: \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Defining the size of the embedding\n",
        "embed_size = 2\n",
        "\n",
        "# Defining the neural network\n",
        "inp = Input(shape=(X.shape[0],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[0], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "# Optimizing the network weights\n",
        "model.fit(\n",
        "    x=X, \n",
        "    y=Y, \n",
        "    batch_size=256,\n",
        "    epochs=100)\n",
        "\n",
        "# Obtaining the weights from the neural network. \n",
        "# These are the so called word embeddings\n",
        "\n",
        "# The input layer \n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
        "# the value is the numeric vector\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "9uSlkxor97Vr",
        "outputId": "3f70f1ee-efe4-4908-fc90-9526473f5103"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-375a01277150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     epochs=100)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Obtaining the weights from the neural network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 10))\n",
        "for word in list(unique_word_dict.keys()):\n",
        "  coord = embedding_dict.get(word)\n",
        "  plt.scatter(coord[0], coord[1])\n",
        "  plt.annotate(word, (coord[0], coord[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "C4Q18ZEM97Rp",
        "outputId": "5f713c92-5522-45ba-ee41-a2d5cdc37c9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-73109f823552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_word_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_dict' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities functions\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def create_unique_word_dict(text:list) -> dict:\n",
        "    \"\"\"\n",
        "    A method that creates a dictionary where the keys are unique words\n",
        "    and key values are indices\n",
        "    \"\"\"\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\n",
        "    words = list(set(text))\n",
        "    words.sort()\n",
        "\n",
        "    # Creating the dictionary for the unique words\n",
        "    unique_word_dict = {}\n",
        "    for i, word in enumerate(words):\n",
        "        unique_word_dict.update({\n",
        "            word: i\n",
        "        })\n",
        "\n",
        "    return unique_word_dict    \n",
        "\n",
        "def text_preprocessing(\n",
        "    text:list,\n",
        "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
        "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
        "    )->list:\n",
        "    \"\"\"\n",
        "    A method to preproces text\n",
        "    \"\"\"\n",
        "    for x in text.lower(): \n",
        "        if x in punctuations: \n",
        "            text = text.replace(x, \"\")\n",
        "\n",
        "    # Removing words that have numbers in them\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # Removing digits\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "\n",
        "    # Cleaning the whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Setting every word to lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Converting all our text to a list \n",
        "    text = text.split(' ')\n",
        "\n",
        "    # Droping empty strings\n",
        "    text = [x for x in text if x!='']\n",
        "\n",
        "    # Droping stop words\n",
        "    text = [x for x in text if x not in stop_words]\n",
        "\n",
        "    return text\n",
        "\n",
        "# Functions to find the most similar word \n",
        "def euclidean(vec1:np.array, vec2:np.array) -> float:\n",
        "    \"\"\"\n",
        "    A function to calculate the euclidean distance between two vectors\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
        "\n",
        "def find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n",
        "    \"\"\"\n",
        "    A method to find the most similar word based on the learnt embeddings\n",
        "    \"\"\"\n",
        "    dist_dict = {}\n",
        "    word_vector = embedding_dict.get(word, [])\n",
        "    if len(word_vector) > 0:\n",
        "        for key, value in embedding_dict.items():\n",
        "            if key!=word:\n",
        "                dist = euclidean(word_vector, value)\n",
        "                dist_dict.update({\n",
        "                    key: dist\n",
        "                })\n",
        "\n",
        "        return sorted(dist_dict.items(), key=lambda x: x[1])[0:top_n]       "
      ],
      "metadata": {
        "id": "f1VxWzB897Oj"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}